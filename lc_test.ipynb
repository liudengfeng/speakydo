{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, List, Mapping, Optional\n",
    "\n",
    "from langchain.callbacks.manager import CallbackManagerForLLMRun\n",
    "from langchain.llms.base import LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pylib.meta_llama2 import get_model_tokenizer, chat_completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetaLLAMA2(LLM):\n",
    "    model: Any\n",
    "    tokenizer: Any\n",
    "    temperature: float\n",
    "    top_p: float\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"custom\"\n",
    "\n",
    "    def _call(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "    ) -> str:\n",
    "        if stop is not None:\n",
    "            raise ValueError(\"stop kwargs are not permitted.\")\n",
    "        return chat_completion(\n",
    "            self.model,\n",
    "            self.tokenizer,\n",
    "            prompt,\n",
    "            [],\n",
    "            None,\n",
    "            temperature=self.temperature,\n",
    "            top_p=self.top_p,\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self) -> Mapping[str, Any]:\n",
    "        \"\"\"Get the identifying parameters.\"\"\"\n",
    "        return {\"model_id\": \"meta-llama/Llama-2-7b-chat-hf\", \"max_new_tokens\": 4096}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72bbef600fc6401b85e31977ae7ba3aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model, tokenizer = get_model_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = MetaLLAMA2(model=model, tokenizer=tokenizer, temperature=0.6, top_p=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" I apologize, but the question you provided doesn't make sense. Justin Bieber was born in 1994, and the Super Bowl did not exist until 1967. Therefore, no NFL team can have won the Super Bowl in the year Justin Bieber was born. Is there anything else I can help you with?\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 17.6s\n",
    "llm(\"What NFL team won the Super Bowl in the year Justin Bieber was born?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mMetaLLAMA2\u001b[0m\n",
      "Params: {'model_id': 'meta-llama/Llama-2-7b-chat-hf', 'max_new_tokens': 4096}\n"
     ]
    }
   ],
   "source": [
    "print(llm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "speakydo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
